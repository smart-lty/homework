# 机器学习作业三：决策树

- 姓名：刘天宇
- 学号：2018312387
- 班级：计算机科学18

本次作业的任务是：

编程实现基于**基尼指数**进行划分选择的决策树算法，为表$4.2$中数据生成预剪枝、后剪枝决策树，并与未剪枝决策树进行比较。

本次作业的文件包括：

- `utils.py`实现各种*辅助函数*
- `trees.py`实现*决策树的生成*、*预剪枝*与*后剪枝*
- `main.py`实现*数据输入输出*

## 基尼指数

基尼指数是用于选择划分属性的方法。

数据集$D$的纯度可用基尼值来度量。
$$
Gini(D)=1-\sum\limits_{k=1}^{|\gamma|}p_k^2
$$
在本题中，$|\gamma|=2$，$p_k=\frac{|D_k|}{|D|},k=1,2$，所以有：
$$
Gini(D)=1-\frac{|D^+|^2+|D^-|^2}{|D|^2}=\frac{2|D^+||D^-|}{|D|^2}
$$
对于某个属性$a$，其基尼指数可以定义为：
$$
Gini\_index(D,a)=\sum\limits_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)=\frac{1}{|D|}\sum\limits_{v=1}^{V}\frac{2|D^{v+}||D^{v-}|}{|D^v|}
$$
基尼指数是用于比较的，所以我们在比较时可以暂时不考虑它们的公共系数，即比较如下所示：
$$
\sum\limits_{v=1}^V\frac{|D^{v+}||D^{v-}|}{|D^v|}
$$

## 决策树生成

使用基尼指数对特征集合依次进行划分。关键代码如下：

```python
def Tree(D, labels, fullDataset):
    """
    生成初始化未剪枝决策树
    :param D: 待处理dataset
    :param labels: 给定标签集合
    :param fullDataset: 总体dataset
    :return: decision tree (type of dict)
    """
    labelName = {0: 'id', 1: '色泽', 2: '根蒂', 3: '敲声', 4: '纹理', 5: '脐部', 6: '触感', 7: '好瓜'}

    if labelClasses(D) == 1:
        # 类别个数为1，返回该类别即可
        return D[0, -1]
    
    if (not labels) or (not checkLabels(D, labels)):
        # 若未给定特征，或label对应的类别是确定的，则返回多数类
        return majorLabels(D)

    # 使用gini指数选择特征
    feature, featureDic = chooseFeature(D, labels)

    # 这里在特征后加入表示该特征对应的多数类
    featureName = '%s%s' % (labelName[feature], majorLabels(D))

    initTree = {featureName: {}}
    featureVal = np.unique(fullDataset[:, feature])

    new_labels = labels[:]
    new_labels.remove(feature)

    for item in featureVal:
        if item not in featureDic.keys():
            # 若某个特征并无对应实例，也要初始化为空
            featureDic[item] = []

    for key, value in featureDic.items():
        if value:
            # 递归
            initTree[featureName][key] = Tree(fullDataset[value, :], new_labels, fullDataset)
        else:
            # 若该属性并无对应实例，即叶子节点，则将其划分为D中最大类别
            initTree[featureName][key] = majorLabels(D)
    return initTree
```

## 预剪枝

在决策树生成的过程中便进行剪枝。若当前节点的划分并不能带来决策树泛化性能提升，则停止划分。

预剪枝能节省大量的时间和空间开销，但其性能未必最优，带来了一些**欠拟合**的风险。

```python
def preTransformTree(labels, trainDataset, testDataset, fullDataset, preCorrect):
    """预剪枝决策树"""

    labelName = {0: 'id', 1: '色泽', 2: '根蒂', 3: '敲声', 4: '纹理', 5: '脐部', 6: '触感', 7: '好瓜'}

    if labelClasses(trainDataset) == 1:
        return trainDataset[0, -1]

    if (not labels) or (not checkLabels(trainDataset, labels)):
        return majorLabels(trainDataset)

    feature, featureDic = chooseFeature(trainDataset, labels)
    featureVal = np.unique(fullDataset[:, feature])

    for item in featureVal:
        if item not in featureDic.keys():
            featureDic[item] = []

    # 比较此节点划分前后的泛化能力
    n, postCorrect = len(testDataset), 0
    nodeLabel = majorLabels(trainDataset)

    judge, test_D_v = {}, {}
    for fea, idx in featureDic.items():
        if not idx:
            judge[fea] = majorLabels(trainDataset)  # 节点的多数类
        else:
            judge[fea] = majorLabels(fullDataset[idx, :])  # 各子集的多数类

    for i in range(n):
        if judge[testDataset[i, feature]] == testDataset[i, -1]:
            postCorrect += 1  # 划分节点后的正确率
        if testDataset[i, feature] not in test_D_v.keys():
            test_D_v[testDataset[i, feature]] = []

        test_D_v[testDataset[i, feature]].append(int(testDataset[i, 0]))

    postCorrect /= n

    if postCorrect <= preCorrect:  # 决定是否剪枝
        return nodeLabel
    else:
        featureName = '%s%s' % (labelName[feature], majorLabels(trainDataset))
        tree = {featureName: {}}
        newLabels = labels[:]
        newLabels.remove(feature)
        for fea, idx in featureDic.items():
            if idx:
                tdv = test_D_v[fea]
                tree[featureName][fea] = preTransformTree(newLabels, fullDataset[idx, :], fullDataset[tdv, :], fullDataset, postCorrect)
            else:
                tree[featureName][fea] = majorLabels(trainDataset)
    return tree
```

## 后剪枝

后剪枝首先生成一颗完整的决策树，然后在此基础上自底向上依次考察非叶节点。若剪枝能带来泛化性能提升，则进行剪枝。

后剪枝通常保留更多的分支，因此**欠拟合**的风险较小。但相应的，它也需要更多的时间与空间开销。

```python
def dataTree(tree, testSet, nameLabel, fullDataset):
    """返回未剪枝决策树tree中，各节点包含的测试集test_D内实例形成的字典"""
    if type(tree).__name__ != 'dict':
        idx = []
        for i in range(len(testSet)):
            idx.append(testSet[i, 0])
        return idx

    feature, featureDic = list(tree.items())[0]
    data_tree = {feature: {}}
    lab = nameLabel[feature[:-1]]
    for key, value in featureDic.items():
        if type(value).__name__ == 'int':
            data_tree[feature][key] = []
            for row in range(len(testSet)):
                if testSet[row, lab] == key:
                    data_tree[feature][key].append(testSet[row, 0])
        else:
            subset = []
            for row in range(len(testSet)):
                if testSet[row, lab] == key:
                    subset.append(testSet[row, 0])
        data_tree[feature][key] = dataTree(value, fullDataset[subset, :], nameLabel, fullDataset)
    return data_tree


def updateTree(data_tree, tree, fullDataset, previousTree=None, previousDataTree=None, previousKey=None):
    """data_tree为测试集在节点中分布的字典，edit_tree为要进行单次剪枝的决策树"""
    feature = list(tree.keys())[0]
    d = data_tree[feature]
    flag, node_data = [], []
    for datum in list(d.values()):
        if type(datum).__name__ == 'dict':
            flag.append(datum)
            break
    if not flag:
        pre, post = 0, 0
        for key, value in d.items():
            if value:
                for i in value:
                    node_data.append(i)
                    if fullDataset[i, -1] == tree[feature][key]:
                        pre += 1
                    if fullDataset[i, -1] == int(feature[-1]):
                        post += 1
        if post > pre and previousTree:
            previousTree[previousKey] = int(feature[-1])
            previousDataTree[previousKey] = node_data
    else:
        for key, value in tree[feature].items():
            new_d_tree = data_tree[feature][key]
            if type(value).__name__ == 'dict' and new_d_tree:
                updateTree(new_d_tree, value, fullDataset, previousTree=tree[feature], previousDataTree=data_tree[feature], previousKey=key)
    return data_tree, tree


def postTree(data_tree, tree, D):
    oldTree = copy.deepcopy(tree)
    dd, newTree = updateTree(data_tree, tree, D)
    if oldTree != newTree:
        return postTree(dd, newTree, D)
    return newTree
```

## 未剪枝、预剪枝、后剪枝的比较

```python
未剪枝：
{'色泽1': {1: {'敲声1': {1: 1, 3: 0, 2: 0}}, 2: {'根蒂1': {1: 1, 2: {'纹理1': {2: 1, 1: 0, 3: 1}}, 3: 1}}, 3: 0}}
accuracy is: 0.2857142857142857
time used:0.0000000000
预剪枝：
{'色泽1': {1: 1, 2: 1, 3: 0}}
accuracy is: 0.5714285714285714
time used:0.000000
后剪枝：
{'色泽1': {1: 1, 2: {'根蒂1': {1: 1, 2: 1, 3: 1}}, 3: 0}}
accuracy is: 0.5714285714285714
time used:0.000000


```

从结果中可以看出：

- 预剪枝与后剪枝均可以提高泛化性能。
- 预剪枝生成的决策树相比于后剪枝来说，少很多分支。
- 理论上来说预剪枝的速度应该很快，但由于计算机运算的速度过快，在小数据集的情况下并没有明显的差距。